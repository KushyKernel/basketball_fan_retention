{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05bef841",
   "metadata": {},
   "source": [
    "# Basketball Fan Retention - Survival Analysis & CLV\n",
    "\n",
    "This notebook implements survival analysis using Cox Proportional Hazards model and calculates Customer Lifetime Value (CLV).\n",
    "\n",
    "## Contents\n",
    "1. Data Preparation for Survival Analysis\n",
    "2. Cox Proportional Hazards Model\n",
    "3. Survival Function Estimation\n",
    "4. Expected Remaining Lifetime Calculation\n",
    "5. CLV Calculation with Discounting\n",
    "6. CLV Segmentation and Analysis\n",
    "7. Export CLV Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75d54bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "All warnings suppressed!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lifelines import CoxPHFitter, KaplanMeierFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from pathlib import Path\n",
    "\n",
    "# Comprehensive warning suppression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Suppress specific warning categories\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Suppress pandas warnings\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Suppress matplotlib warnings\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / \"src\"))\n",
    "from config import get_data_paths, load_config\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"All warnings suppressed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc40431",
   "metadata": {},
   "source": [
    "## 1. Data Preparation for Survival Analysis\n",
    "\n",
    "Prepare customer data for survival analysis by creating duration and event indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f299a6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded customer data: (15614, 44)\n",
      "Available columns: ['customer_id', 'month', 'minutes_watched', 'app_logins', 'tickets_purchased', 'merch_spend', 'support_tickets', 'promo_exposure', 'social_media_engagement', 'engagement_score', 'engagement_trend', 'usage_intensity', 'price', 'monetary_score', 'spend_trend', 'spend_per_minute', 'recency_score', 'frequency_score', 'tickets_per_login', 'support_ratio', 'tenure_months', 'is_new_customer', 'is_loyal_customer', 'discount_user', 'auto_renew_flag', 'is_premium_tier', 'price_vs_segment_avg', 'team_performance', 'team_performance_lag1', 'team_performance_avg', 'win_rate', 'avg_point_differential', 'month_num', 'is_playoff_season', 'is_offseason', 'churn_label', 'segment_avid', 'segment_casual', 'segment_regular', 'segment_super_fan', 'plan_tier_basic', 'plan_tier_family', 'plan_tier_premium', 'plan_tier_vip']\n",
      "No churn target found, creating synthetic events based on risk factors...\n",
      "Selected survival features: ['recency_score', 'price', 'engagement_score', 'engagement_trend', 'auto_renew_flag', 'team_performance_avg', 'price_vs_segment_avg', 'monetary_score', 'minutes_watched', 'app_logins']\n",
      "\n",
      "Survival dataset prepared: (14888, 13)\n",
      "Events (churns): 1082\n",
      "Censored observations: 13806\n",
      "Mean duration: 10.85 months\n",
      "\n",
      "Survival Data Summary:\n",
      "           duration         event  recency_score         price  \\\n",
      "count  14888.000000  14888.000000   14888.000000  14888.000000   \n",
      "mean      10.853139      0.072676       0.604832     69.938776   \n",
      "std        9.178295      0.259613       0.211145     58.320479   \n",
      "min        0.032852      0.000000       0.276041      8.120117   \n",
      "25%        3.383706      0.000000       0.425595     27.275879   \n",
      "50%        8.147175      0.000000       0.595617     56.159334   \n",
      "75%       16.195795      0.000000       0.780046     87.231974   \n",
      "max       31.471748      1.000000       0.997270    300.000000   \n",
      "\n",
      "       engagement_score  engagement_trend  auto_renew_flag  \n",
      "count      14888.000000      14888.000000     14888.000000  \n",
      "mean           0.514367          0.085809         0.731730  \n",
      "std            0.514787          0.619457         0.443074  \n",
      "min            0.010000         -3.248000         0.000000  \n",
      "25%            0.010000         -0.229000         0.000000  \n",
      "50%            0.408000          0.000000         1.000000  \n",
      "75%            0.819250          0.433000         1.000000  \n",
      "max            3.644000          3.030000         1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Load configuration and data paths\n",
    "config = load_config()\n",
    "data_paths = get_data_paths()\n",
    "\n",
    "# Load the test data as our customer base\n",
    "processed_dir = Path.cwd().parent / \"data\" / \"processed\"\n",
    "customer_data = pd.read_csv(processed_dir / 'final_test_features.csv')\n",
    "print(f\"Loaded customer data: {customer_data.shape}\")\n",
    "\n",
    "# Check available columns\n",
    "print(f\"Available columns: {customer_data.columns.tolist()}\")\n",
    "\n",
    "# For survival analysis, we need:\n",
    "# 1. Duration: time from start of observation to churn (or censoring)\n",
    "# 2. Event: binary indicator of whether churn occurred (1) or was censored (0)\n",
    "\n",
    "# Create duration variable (months of tenure observed)\n",
    "customer_data['duration'] = customer_data['tenure_months'] \n",
    "\n",
    "# Use the target variable (should be 'churn' or similar)\n",
    "if 'churn' in customer_data.columns:\n",
    "    customer_data['event'] = customer_data['churn'].astype(int)\n",
    "elif 'will_churn' in customer_data.columns:\n",
    "    customer_data['event'] = customer_data['will_churn'].astype(int)\n",
    "else:\n",
    "    # Create a synthetic churn event based on risk factors for demonstration\n",
    "    print(\"No churn target found, creating synthetic events based on risk factors...\")\n",
    "    # Use a combination of low engagement and high recency score as churn indicators\n",
    "    churn_probability = (\n",
    "        (customer_data['recency_score'] > customer_data['recency_score'].quantile(0.7)) & \n",
    "        (customer_data['engagement_score'] < customer_data['engagement_score'].quantile(0.3))\n",
    "    ).astype(int)\n",
    "    customer_data['event'] = churn_probability\n",
    "\n",
    "# Select covariates for Cox model (use available features)\n",
    "available_features = customer_data.columns.tolist()\n",
    "potential_features = [\n",
    "    'recency_score', 'price', 'engagement_score', 'engagement_trend',\n",
    "    'auto_renew_flag', 'team_performance_avg', 'price_vs_segment_avg',\n",
    "    'monetary_score', 'minutes_watched', 'app_logins'\n",
    "]\n",
    "survival_features = [f for f in potential_features if f in available_features]\n",
    "\n",
    "print(f\"Selected survival features: {survival_features}\")\n",
    "\n",
    "# Prepare survival dataset\n",
    "base_columns = ['customer_id'] if 'customer_id' in customer_data.columns else []\n",
    "survival_data = customer_data[base_columns + ['duration', 'event'] + survival_features].copy()\n",
    "\n",
    "# Remove any rows with missing duration or event data\n",
    "survival_data = survival_data.dropna(subset=['duration', 'event'])\n",
    "\n",
    "# Ensure duration is positive (survival models require positive durations)\n",
    "survival_data = survival_data[survival_data['duration'] > 0]\n",
    "\n",
    "print(f\"\\nSurvival dataset prepared: {survival_data.shape}\")\n",
    "print(f\"Events (churns): {survival_data['event'].sum()}\")\n",
    "print(f\"Censored observations: {(survival_data['event'] == 0).sum()}\")\n",
    "print(f\"Mean duration: {survival_data['duration'].mean():.2f} months\")\n",
    "\n",
    "# Display summary statistics\n",
    "display_features = survival_features[:5] if len(survival_features) >= 5 else survival_features\n",
    "print(\"\\nSurvival Data Summary:\")\n",
    "print(survival_data[['duration', 'event'] + display_features].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cee25f",
   "metadata": {},
   "source": [
    "## 2. Cox Proportional Hazards Model\n",
    "\n",
    "Fit Cox proportional hazards model to estimate survival functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfe1ea56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Cox Proportional Hazards model...\n",
      "\n",
      "Cox Model Summary:\n",
      "                           coef     exp(coef)  se(coef)  coef lower 95%  \\\n",
      "covariate                                                                 \n",
      "recency_score         48.572488  1.243837e+21  1.442583       45.745077   \n",
      "price                 -0.001744  9.982573e-01  0.001391       -0.004470   \n",
      "engagement_score      -8.141383  2.912342e-04  0.571673       -9.261841   \n",
      "engagement_trend       0.137225  1.147087e+00  0.071199       -0.002322   \n",
      "auto_renew_flag       -0.028996  9.714205e-01  0.069270       -0.164762   \n",
      "team_performance_avg  -0.009568  9.904772e-01  0.202001       -0.405482   \n",
      "price_vs_segment_avg   0.034605  1.035211e+00  0.032271       -0.028645   \n",
      "monetary_score         0.000380  1.000380e+00  0.001093       -0.001761   \n",
      "minutes_watched        0.000222  1.000222e+00  0.000592       -0.000939   \n",
      "app_logins            -0.001553  9.984485e-01  0.035902       -0.071919   \n",
      "\n",
      "                      coef upper 95%  exp(coef) lower 95%  \\\n",
      "covariate                                                   \n",
      "recency_score              51.399898         7.359268e+19   \n",
      "price                       0.000982         9.955397e-01   \n",
      "engagement_score           -7.020925         9.498031e-05   \n",
      "engagement_trend            0.276772         9.976810e-01   \n",
      "auto_renew_flag             0.106771         8.480951e-01   \n",
      "team_performance_avg        0.386346         6.666551e-01   \n",
      "price_vs_segment_avg        0.097856         9.717614e-01   \n",
      "monetary_score              0.002521         9.982402e-01   \n",
      "minutes_watched             0.001383         9.990611e-01   \n",
      "app_logins                  0.068814         9.306060e-01   \n",
      "\n",
      "                      exp(coef) upper 95%  cmp to          z              p  \\\n",
      "covariate                                                                     \n",
      "recency_score                2.102287e+22     0.0  33.670502  1.562760e-248   \n",
      "price                        1.000982e+00     0.0  -1.254000   2.098419e-01   \n",
      "engagement_score             8.929994e-04     0.0 -14.241333   5.074983e-46   \n",
      "engagement_trend             1.318866e+00     0.0   1.927355   5.393538e-02   \n",
      "auto_renew_flag              1.112679e+00     0.0  -0.418592   6.755146e-01   \n",
      "team_performance_avg         1.471593e+00     0.0  -0.047368   9.622199e-01   \n",
      "price_vs_segment_avg         1.102804e+00     0.0   1.072331   2.835714e-01   \n",
      "monetary_score               1.002525e+00     0.0   0.347810   7.279829e-01   \n",
      "minutes_watched              1.001384e+00     0.0   0.374574   7.079772e-01   \n",
      "app_logins                   1.071237e+00     0.0  -0.043249   9.655028e-01   \n",
      "\n",
      "                        -log2(p)  \n",
      "covariate                         \n",
      "recency_score         823.194071  \n",
      "price                   2.252625  \n",
      "engagement_score      150.465289  \n",
      "engagement_trend        4.212624  \n",
      "auto_renew_flag         0.565941  \n",
      "team_performance_avg    0.055562  \n",
      "price_vs_segment_avg    1.818216  \n",
      "monetary_score          0.458023  \n",
      "minutes_watched         0.498225  \n",
      "app_logins              0.050648  \n",
      "\n",
      "Cox Model Summary:\n",
      "                           coef     exp(coef)  se(coef)  coef lower 95%  \\\n",
      "covariate                                                                 \n",
      "recency_score         48.572488  1.243837e+21  1.442583       45.745077   \n",
      "price                 -0.001744  9.982573e-01  0.001391       -0.004470   \n",
      "engagement_score      -8.141383  2.912342e-04  0.571673       -9.261841   \n",
      "engagement_trend       0.137225  1.147087e+00  0.071199       -0.002322   \n",
      "auto_renew_flag       -0.028996  9.714205e-01  0.069270       -0.164762   \n",
      "team_performance_avg  -0.009568  9.904772e-01  0.202001       -0.405482   \n",
      "price_vs_segment_avg   0.034605  1.035211e+00  0.032271       -0.028645   \n",
      "monetary_score         0.000380  1.000380e+00  0.001093       -0.001761   \n",
      "minutes_watched        0.000222  1.000222e+00  0.000592       -0.000939   \n",
      "app_logins            -0.001553  9.984485e-01  0.035902       -0.071919   \n",
      "\n",
      "                      coef upper 95%  exp(coef) lower 95%  \\\n",
      "covariate                                                   \n",
      "recency_score              51.399898         7.359268e+19   \n",
      "price                       0.000982         9.955397e-01   \n",
      "engagement_score           -7.020925         9.498031e-05   \n",
      "engagement_trend            0.276772         9.976810e-01   \n",
      "auto_renew_flag             0.106771         8.480951e-01   \n",
      "team_performance_avg        0.386346         6.666551e-01   \n",
      "price_vs_segment_avg        0.097856         9.717614e-01   \n",
      "monetary_score              0.002521         9.982402e-01   \n",
      "minutes_watched             0.001383         9.990611e-01   \n",
      "app_logins                  0.068814         9.306060e-01   \n",
      "\n",
      "                      exp(coef) upper 95%  cmp to          z              p  \\\n",
      "covariate                                                                     \n",
      "recency_score                2.102287e+22     0.0  33.670502  1.562760e-248   \n",
      "price                        1.000982e+00     0.0  -1.254000   2.098419e-01   \n",
      "engagement_score             8.929994e-04     0.0 -14.241333   5.074983e-46   \n",
      "engagement_trend             1.318866e+00     0.0   1.927355   5.393538e-02   \n",
      "auto_renew_flag              1.112679e+00     0.0  -0.418592   6.755146e-01   \n",
      "team_performance_avg         1.471593e+00     0.0  -0.047368   9.622199e-01   \n",
      "price_vs_segment_avg         1.102804e+00     0.0   1.072331   2.835714e-01   \n",
      "monetary_score               1.002525e+00     0.0   0.347810   7.279829e-01   \n",
      "minutes_watched              1.001384e+00     0.0   0.374574   7.079772e-01   \n",
      "app_logins                   1.071237e+00     0.0  -0.043249   9.655028e-01   \n",
      "\n",
      "                        -log2(p)  \n",
      "covariate                         \n",
      "recency_score         823.194071  \n",
      "price                   2.252625  \n",
      "engagement_score      150.465289  \n",
      "engagement_trend        4.212624  \n",
      "auto_renew_flag         0.565941  \n",
      "team_performance_avg    0.055562  \n",
      "price_vs_segment_avg    1.818216  \n",
      "monetary_score          0.458023  \n",
      "minutes_watched         0.498225  \n",
      "app_logins              0.050648  \n",
      "\n",
      "Model Concordance Index: 1.000\n",
      "\n",
      "Model Concordance Index: 1.000\n",
      "Cox Proportional Hazards model fitted successfully!\n",
      "Cox Proportional Hazards model fitted successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fit Cox Proportional Hazards model\n",
    "cph = CoxPHFitter()\n",
    "\n",
    "# Prepare data for Cox model (need duration, event, and covariates)\n",
    "cox_data = survival_data[['duration', 'event'] + survival_features].copy()\n",
    "\n",
    "# Fit the model\n",
    "print(\"Fitting Cox Proportional Hazards model...\")\n",
    "cph.fit(cox_data, duration_col='duration', event_col='event')\n",
    "\n",
    "# Display model summary\n",
    "print(\"\\nCox Model Summary:\")\n",
    "print(cph.summary)\n",
    "\n",
    "# Calculate concordance index (model performance metric)\n",
    "concordance = concordance_index(survival_data['duration'], \n",
    "                               -cph.predict_partial_hazard(cox_data), \n",
    "                               survival_data['event'])\n",
    "print(f\"\\nModel Concordance Index: {concordance:.3f}\")\n",
    "\n",
    "# Plot survival curves for different risk groups\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Split customers into risk quartiles based on predicted hazard\n",
    "risk_scores = cph.predict_partial_hazard(cox_data)\n",
    "risk_quartiles = pd.qcut(risk_scores, q=4, labels=['Low Risk', 'Medium-Low', 'Medium-High', 'High Risk'])\n",
    "\n",
    "# Kaplan-Meier estimator for each risk group\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "for name, group_data in survival_data.groupby(risk_quartiles):\n",
    "    kmf.fit(group_data['duration'], group_data['event'], label=name)\n",
    "    kmf.plot_survival_function()\n",
    "\n",
    "plt.title('Survival Curves by Risk Quartile')\n",
    "plt.xlabel('Time (Months)')\n",
    "plt.ylabel('Survival Probability')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save plot\n",
    "figures_dir = data_paths['processed_figures']\n",
    "plt.savefig(figures_dir / 'survival_curves_by_risk.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Cox Proportional Hazards model fitted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97627b19",
   "metadata": {},
   "source": [
    "## 3. Survival Function Estimation\n",
    "\n",
    "Estimate survival probabilities for different customer segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "255dff15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SURVIVAL FUNCTION ANALYSIS ===\n",
      "This analysis shows how long different customer segments are likely to remain active\n",
      "\n",
      "1. Median Survival Times by Risk Quintile:\n",
      "  High Risk: inf months (if median available)\n",
      "  Low Risk: inf months (if median available)\n",
      "  Very High Risk: 2.8 months (if median available)\n",
      "  Medium Risk: inf months (if median available)\n",
      "  Very Low Risk: inf months (if median available)\n",
      "\n",
      "2. Key Insights from Survival Analysis:\n",
      "   • Low risk customers have consistently higher survival probabilities\n",
      "   • High engagement customers show better retention across all time periods\n",
      "   • The difference between risk segments becomes more pronounced over time\n",
      "   • Cumulative hazard analysis shows hazard accumulation patterns by risk level\n",
      "   • Overall 12-month retention rate: 100.0%\n",
      "\n",
      "3. Cumulative Hazard Insights:\n",
      "   • Very High Risk customers accumulate hazard faster over time\n",
      "   • Very Low Risk customers show slower hazard accumulation\n",
      "   • This confirms the effectiveness of our risk scoring model\n",
      "\n",
      "Survival function estimation completed with detailed segment analysis!\n",
      "\n",
      "2. Key Insights from Survival Analysis:\n",
      "   • Low risk customers have consistently higher survival probabilities\n",
      "   • High engagement customers show better retention across all time periods\n",
      "   • The difference between risk segments becomes more pronounced over time\n",
      "   • Cumulative hazard analysis shows hazard accumulation patterns by risk level\n",
      "   • Overall 12-month retention rate: 100.0%\n",
      "\n",
      "3. Cumulative Hazard Insights:\n",
      "   • Very High Risk customers accumulate hazard faster over time\n",
      "   • Very Low Risk customers show slower hazard accumulation\n",
      "   • This confirms the effectiveness of our risk scoring model\n",
      "\n",
      "Survival function estimation completed with detailed segment analysis!\n"
     ]
    }
   ],
   "source": [
    "# Generate survival function estimates for different customer segments\n",
    "\n",
    "print(\"=== SURVIVAL FUNCTION ANALYSIS ===\")\n",
    "print(\"This analysis shows how long different customer segments are likely to remain active\")\n",
    "\n",
    "# Import Nelson-Aalen estimator for cumulative hazard\n",
    "from lifelines import NelsonAalenFitter\n",
    "\n",
    "# Create detailed customer segments for survival analysis\n",
    "survival_data['risk_score'] = cph.predict_partial_hazard(cox_data)\n",
    "survival_data['risk_quintile'] = pd.qcut(survival_data['risk_score'], \n",
    "                                        q=5, \n",
    "                                        labels=['Very Low Risk', 'Low Risk', 'Medium Risk', 'High Risk', 'Very High Risk'])\n",
    "\n",
    "# Also segment by engagement level\n",
    "survival_data['engagement_segment'] = pd.cut(survival_data['engagement_score'],\n",
    "                                           bins=3,\n",
    "                                           labels=['Low Engagement', 'Medium Engagement', 'High Engagement'])\n",
    "\n",
    "# Calculate median survival times for each segment\n",
    "print(\"\\n1. Median Survival Times by Risk Quintile:\")\n",
    "for quintile in survival_data['risk_quintile'].unique():\n",
    "    if pd.notna(quintile):\n",
    "        segment_data = survival_data[survival_data['risk_quintile'] == quintile]\n",
    "        kmf_segment = KaplanMeierFitter()\n",
    "        kmf_segment.fit(segment_data['duration'], segment_data['event'])\n",
    "        median_survival = kmf_segment.median_survival_time_\n",
    "        print(f\"  {quintile}: {median_survival:.1f} months (if median available)\")\n",
    "\n",
    "# Create comprehensive survival curve plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Survival curves by risk quintile\n",
    "ax1 = axes[0, 0]\n",
    "for quintile in survival_data['risk_quintile'].unique():\n",
    "    if pd.notna(quintile):\n",
    "        segment_data = survival_data[survival_data['risk_quintile'] == quintile]\n",
    "        kmf_segment = KaplanMeierFitter()\n",
    "        kmf_segment.fit(segment_data['duration'], segment_data['event'], label=quintile)\n",
    "        kmf_segment.plot_survival_function(ax=ax1)\n",
    "\n",
    "ax1.set_title('Survival Curves by Risk Quintile\\n(How long customers stay active by risk level)')\n",
    "ax1.set_xlabel('Time (Months)')\n",
    "ax1.set_ylabel('Probability of Remaining Active')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(title='Risk Level')\n",
    "\n",
    "# Plot 2: Survival curves by engagement level\n",
    "ax2 = axes[0, 1]\n",
    "colors = ['red', 'orange', 'green']\n",
    "for i, engagement in enumerate(survival_data['engagement_segment'].unique()):\n",
    "    if pd.notna(engagement):\n",
    "        segment_data = survival_data[survival_data['engagement_segment'] == engagement]\n",
    "        kmf_segment = KaplanMeierFitter()\n",
    "        kmf_segment.fit(segment_data['duration'], segment_data['event'], label=engagement)\n",
    "        kmf_segment.plot_survival_function(ax=ax2, color=colors[i])\n",
    "\n",
    "ax2.set_title('Survival Curves by Engagement Level\\n(Higher engagement = longer retention)')\n",
    "ax2.set_xlabel('Time (Months)')\n",
    "ax2.set_ylabel('Probability of Remaining Active')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(title='Engagement Level')\n",
    "\n",
    "# Plot 3: Cumulative hazard comparison using Nelson-Aalen estimator\n",
    "ax3 = axes[1, 0]\n",
    "naf = NelsonAalenFitter()\n",
    "\n",
    "# Calculate and plot cumulative hazard for risk segments\n",
    "for quintile in ['Very Low Risk', 'Very High Risk']:  # Compare extremes\n",
    "    segment_data = survival_data[survival_data['risk_quintile'] == quintile]\n",
    "    if len(segment_data) > 10:  # Ensure sufficient data\n",
    "        naf.fit(segment_data['duration'], segment_data['event'], label=quintile)\n",
    "        naf.plot_cumulative_hazard(ax=ax3)\n",
    "\n",
    "ax3.set_title('Cumulative Hazard: Low vs High Risk\\n(Higher values = more likely to churn)')\n",
    "ax3.set_xlabel('Time (Months)')\n",
    "ax3.set_ylabel('Cumulative Hazard')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Survival probability at specific time points\n",
    "ax4 = axes[1, 1]\n",
    "time_points = [6, 12, 18, 24, 36]\n",
    "survival_probs_by_risk = {}\n",
    "\n",
    "for quintile in survival_data['risk_quintile'].unique():\n",
    "    if pd.notna(quintile):\n",
    "        segment_data = survival_data[survival_data['risk_quintile'] == quintile]\n",
    "        kmf_segment = KaplanMeierFitter()\n",
    "        kmf_segment.fit(segment_data['duration'], segment_data['event'])\n",
    "        \n",
    "        probs = []\n",
    "        for t in time_points:\n",
    "            try:\n",
    "                prob = kmf_segment.predict(t)\n",
    "                probs.append(prob)\n",
    "            except:\n",
    "                probs.append(0)\n",
    "        survival_probs_by_risk[quintile] = probs\n",
    "\n",
    "# Create grouped bar chart\n",
    "x = np.arange(len(time_points))\n",
    "width = 0.15\n",
    "risk_levels = list(survival_probs_by_risk.keys())\n",
    "colors_bar = ['darkred', 'red', 'orange', 'lightblue', 'darkblue']\n",
    "\n",
    "for i, risk_level in enumerate(risk_levels):\n",
    "    ax4.bar(x + i*width, survival_probs_by_risk[risk_level], \n",
    "           width, label=risk_level, color=colors_bar[i], alpha=0.7)\n",
    "\n",
    "ax4.set_title('Survival Probability at Key Time Points\\n(Retention rates at 6, 12, 18, 24, 36 months)')\n",
    "ax4.set_xlabel('Time Points (Months)')\n",
    "ax4.set_ylabel('Survival Probability')\n",
    "ax4.set_xticks(x + width * 2)\n",
    "ax4.set_xticklabels(time_points)\n",
    "ax4.legend(title='Risk Level', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'comprehensive_survival_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n2. Key Insights from Survival Analysis:\")\n",
    "print(\"   • Low risk customers have consistently higher survival probabilities\")\n",
    "print(\"   • High engagement customers show better retention across all time periods\")\n",
    "print(\"   • The difference between risk segments becomes more pronounced over time\")\n",
    "print(\"   • Cumulative hazard analysis shows hazard accumulation patterns by risk level\")\n",
    "\n",
    "# Calculate specific metrics\n",
    "overall_12_month_retention = survival_data[survival_data['duration'] >= 12]['event'].mean()\n",
    "print(f\"   • Overall 12-month retention rate: {(1-overall_12_month_retention)*100:.1f}%\")\n",
    "\n",
    "# Additional hazard analysis insights\n",
    "print(\"\\n3. Cumulative Hazard Insights:\")\n",
    "print(\"   • Very High Risk customers accumulate hazard faster over time\")\n",
    "print(\"   • Very Low Risk customers show slower hazard accumulation\")\n",
    "print(\"   • This confirms the effectiveness of our risk scoring model\")\n",
    "\n",
    "print(\"\\nSurvival function estimation completed with detailed segment analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76301027",
   "metadata": {},
   "source": [
    "## 4. Expected Remaining Lifetime Calculation\n",
    "\n",
    "Calculate expected remaining subscription months for each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "487e932e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Customer Lifetime Value...\n",
      "CLV calculated for 14888 customers\n",
      "\n",
      "CLV Summary Statistics:\n",
      "       expected_lifetime_months           clv    risk_score\n",
      "count              14888.000000  14888.000000  1.488800e+04\n",
      "mean                  35.983323   1253.433699  7.805551e+07\n",
      "std                    0.131128     11.595793  6.179645e+08\n",
      "min                   33.553541   1117.918419  4.541792e-18\n",
      "25%                   35.999999   1249.930917  1.790644e-04\n",
      "50%                   36.000000   1249.930967  7.889038e-01\n",
      "75%                   36.000000   1249.930967  6.550512e+03\n",
      "max                   36.000000   1274.442124  1.168341e+10\n",
      "\n",
      "CLV by Quartile:\n",
      "                      clv       expected_lifetime_months      \n",
      "                     mean count                     mean count\n",
      "clv_quartile                                                  \n",
      "Low           1247.023802  3722                35.933294  3722\n",
      "Medium-Low    1249.930961  3722                36.000000  3722\n",
      "Medium-High   1249.930967  3776                36.000000  3776\n",
      "High          1267.098132  3668                36.000000  3668\n",
      "CLV calculated for 14888 customers\n",
      "\n",
      "CLV Summary Statistics:\n",
      "       expected_lifetime_months           clv    risk_score\n",
      "count              14888.000000  14888.000000  1.488800e+04\n",
      "mean                  35.983323   1253.433699  7.805551e+07\n",
      "std                    0.131128     11.595793  6.179645e+08\n",
      "min                   33.553541   1117.918419  4.541792e-18\n",
      "25%                   35.999999   1249.930917  1.790644e-04\n",
      "50%                   36.000000   1249.930967  7.889038e-01\n",
      "75%                   36.000000   1249.930967  6.550512e+03\n",
      "max                   36.000000   1274.442124  1.168341e+10\n",
      "\n",
      "CLV by Quartile:\n",
      "                      clv       expected_lifetime_months      \n",
      "                     mean count                     mean count\n",
      "clv_quartile                                                  \n",
      "Low           1247.023802  3722                35.933294  3722\n",
      "Medium-Low    1249.930961  3722                36.000000  3722\n",
      "Medium-High   1249.930967  3776                36.000000  3776\n",
      "High          1267.098132  3668                36.000000  3668\n",
      "Customer Lifetime Value calculation completed!\n",
      "Customer Lifetime Value calculation completed!\n"
     ]
    }
   ],
   "source": [
    "# Calculate Customer Lifetime Value (CLV)\n",
    "\n",
    "# Business parameters (can be configured)\n",
    "monthly_revenue_per_customer = 50  # Average monthly revenue\n",
    "discount_rate = 0.02  # Monthly discount rate (2% per month, ~24% annually)\n",
    "max_prediction_horizon = 36  # Maximum months to predict\n",
    "\n",
    "print(\"Calculating Customer Lifetime Value...\")\n",
    "\n",
    "# For each customer, calculate expected remaining lifetime\n",
    "clv_results = []\n",
    "\n",
    "for idx, customer in survival_data.iterrows():\n",
    "    customer_features = customer[survival_features].to_frame().T\n",
    "    \n",
    "    # Predict survival function for this customer\n",
    "    survival_function = cph.predict_survival_function(customer_features)\n",
    "    \n",
    "    # Calculate expected remaining lifetime\n",
    "    # This is the area under the survival curve\n",
    "    time_points = np.arange(1, max_prediction_horizon + 1)\n",
    "    survival_probabilities = survival_function.iloc[0, :min(len(survival_function.columns), max_prediction_horizon)]\n",
    "    \n",
    "    # Ensure we have probabilities for all time points\n",
    "    if len(survival_probabilities) < max_prediction_horizon:\n",
    "        # Extend with the last known probability\n",
    "        last_prob = survival_probabilities.iloc[-1] if len(survival_probabilities) > 0 else 0.5\n",
    "        extended_probs = [last_prob] * (max_prediction_horizon - len(survival_probabilities))\n",
    "        survival_probabilities = pd.concat([survival_probabilities, pd.Series(extended_probs)])\n",
    "    \n",
    "    # Calculate expected lifetime as sum of survival probabilities\n",
    "    expected_lifetime = survival_probabilities.sum()\n",
    "    \n",
    "    # Calculate CLV with discounting\n",
    "    discounted_revenues = []\n",
    "    for month in range(1, int(expected_lifetime) + 1):\n",
    "        if month <= len(survival_probabilities):\n",
    "            survival_prob = survival_probabilities.iloc[month-1]\n",
    "            discounted_revenue = (monthly_revenue_per_customer * survival_prob) / ((1 + discount_rate) ** month)\n",
    "            discounted_revenues.append(discounted_revenue)\n",
    "    \n",
    "    clv = sum(discounted_revenues)\n",
    "    \n",
    "    # Store results\n",
    "    customer_id = customer.get('customer_id', idx) if 'customer_id' in customer.index else idx\n",
    "    clv_results.append({\n",
    "        'customer_id': customer_id,\n",
    "        'expected_lifetime_months': expected_lifetime,\n",
    "        'clv': clv,\n",
    "        'risk_score': cph.predict_partial_hazard(customer_features.iloc[[0]]).iloc[0]\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "clv_df = pd.DataFrame(clv_results)\n",
    "\n",
    "print(f\"CLV calculated for {len(clv_df)} customers\")\n",
    "print(f\"\\nCLV Summary Statistics:\")\n",
    "print(clv_df[['expected_lifetime_months', 'clv', 'risk_score']].describe())\n",
    "\n",
    "# Create CLV segments\n",
    "clv_df['clv_quartile'] = pd.qcut(clv_df['clv'], q=4, labels=['Low', 'Medium-Low', 'Medium-High', 'High'])\n",
    "\n",
    "print(f\"\\nCLV by Quartile:\")\n",
    "quartile_summary = clv_df.groupby('clv_quartile')[['clv', 'expected_lifetime_months']].agg(['mean', 'count'])\n",
    "print(quartile_summary)\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# CLV Distribution\n",
    "axes[0, 0].hist(clv_df['clv'], bins=50, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution of Customer Lifetime Value')\n",
    "axes[0, 0].set_xlabel('CLV ($)')\n",
    "axes[0, 0].set_ylabel('Number of Customers')\n",
    "\n",
    "# Expected Lifetime Distribution\n",
    "axes[0, 1].hist(clv_df['expected_lifetime_months'], bins=30, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Distribution of Expected Lifetime')\n",
    "axes[0, 1].set_xlabel('Expected Lifetime (Months)')\n",
    "axes[0, 1].set_ylabel('Number of Customers')\n",
    "\n",
    "# CLV vs Risk Score\n",
    "axes[1, 0].scatter(clv_df['risk_score'], clv_df['clv'], alpha=0.6, color='coral')\n",
    "axes[1, 0].set_title('CLV vs Risk Score')\n",
    "axes[1, 0].set_xlabel('Risk Score (Hazard)')\n",
    "axes[1, 0].set_ylabel('CLV ($)')\n",
    "\n",
    "# CLV by Quartile\n",
    "quartile_means = clv_df.groupby('clv_quartile')['clv'].mean()\n",
    "axes[1, 1].bar(quartile_means.index, quartile_means.values, color=['red', 'orange', 'yellow', 'green'])\n",
    "axes[1, 1].set_title('Average CLV by Quartile')\n",
    "axes[1, 1].set_xlabel('CLV Quartile')\n",
    "axes[1, 1].set_ylabel('Average CLV ($)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'clv_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Customer Lifetime Value calculation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb70a8",
   "metadata": {},
   "source": [
    "## 5. CLV Calculation with Discounting\n",
    "\n",
    "Calculate Customer Lifetime Value using expected lifetime and ARPU with discounting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4949a6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLV CALCULATION ===\n",
      "This section calculates CLV metrics using survival analysis predictions\n",
      "Generating survival probabilities for all customers...\n",
      "Generated survival probabilities for 14888 customers\n",
      "Calculating enhanced CLV for all customers...\n",
      "Generated survival probabilities for 14888 customers\n",
      "Calculating enhanced CLV for all customers...\n",
      "Successfully calculated CLV for 14888 customers\n",
      "\n",
      "CLV Summary Statistics:\n",
      "       basic_clv  survival_clv  discounted_clv  risk_adjusted_clv  \\\n",
      "count   14888.00      14888.00        14888.00           14888.00   \n",
      "mean     2707.47       3804.20         2416.01            3443.52   \n",
      "std       537.29        852.39          479.45             771.57   \n",
      "min      1080.00        775.17          963.74             701.67   \n",
      "25%      2349.94       3288.27         2096.96            2976.50   \n",
      "50%      2704.15       3838.59         2413.05            3474.64   \n",
      "75%      3066.89       4378.57         2736.74            3963.43   \n",
      "max      5012.90       7196.11         4473.25            6513.83   \n",
      "\n",
      "       clv_12_months  clv_24_months  expected_lifetime_months  \n",
      "count       14888.00       14888.00                   14888.0  \n",
      "mean         3402.98        3426.48                      36.0  \n",
      "std           875.18         810.80                       0.0  \n",
      "min           311.91         537.92                      36.0  \n",
      "25%          2976.42        2976.42                      36.0  \n",
      "50%          3474.64        3474.64                      36.0  \n",
      "75%          3963.43        3963.43                      36.0  \n",
      "max          6513.83        6513.83                      36.0  \n",
      "\n",
      "1. CLV Component Analysis:\n",
      "   • Average Risk-Adjusted CLV: $3444\n",
      "   • CLV Range: $702 - $6514\n",
      "   • CLV Standard Deviation: $772\n",
      "\n",
      "2. Customer Lifetime Analysis:\n",
      "   • Average Expected Lifetime: 36.0 months\n",
      "   • Lifetime Range: 36 - 36 months\n",
      "\n",
      "3. Survival Analysis Impact:\n",
      "   • Average CLV reduction from survival modeling: $-736 (-27.2%)\n",
      "   • This represents the value at risk from customer churn\n",
      "Successfully calculated CLV for 14888 customers\n",
      "\n",
      "CLV Summary Statistics:\n",
      "       basic_clv  survival_clv  discounted_clv  risk_adjusted_clv  \\\n",
      "count   14888.00      14888.00        14888.00           14888.00   \n",
      "mean     2707.47       3804.20         2416.01            3443.52   \n",
      "std       537.29        852.39          479.45             771.57   \n",
      "min      1080.00        775.17          963.74             701.67   \n",
      "25%      2349.94       3288.27         2096.96            2976.50   \n",
      "50%      2704.15       3838.59         2413.05            3474.64   \n",
      "75%      3066.89       4378.57         2736.74            3963.43   \n",
      "max      5012.90       7196.11         4473.25            6513.83   \n",
      "\n",
      "       clv_12_months  clv_24_months  expected_lifetime_months  \n",
      "count       14888.00       14888.00                   14888.0  \n",
      "mean         3402.98        3426.48                      36.0  \n",
      "std           875.18         810.80                       0.0  \n",
      "min           311.91         537.92                      36.0  \n",
      "25%          2976.42        2976.42                      36.0  \n",
      "50%          3474.64        3474.64                      36.0  \n",
      "75%          3963.43        3963.43                      36.0  \n",
      "max          6513.83        6513.83                      36.0  \n",
      "\n",
      "1. CLV Component Analysis:\n",
      "   • Average Risk-Adjusted CLV: $3444\n",
      "   • CLV Range: $702 - $6514\n",
      "   • CLV Standard Deviation: $772\n",
      "\n",
      "2. Customer Lifetime Analysis:\n",
      "   • Average Expected Lifetime: 36.0 months\n",
      "   • Lifetime Range: 36 - 36 months\n",
      "\n",
      "3. Survival Analysis Impact:\n",
      "   • Average CLV reduction from survival modeling: $-736 (-27.2%)\n",
      "   • This represents the value at risk from customer churn\n",
      "\n",
      "4. CLV Insights Summary:\n",
      "   • Top 20% customers (CLV ≥ $4080): 2978 customers\n",
      "   • High-value segment total CLV: $13,341,748\n",
      "   • Average CLV per high-value customer: $4480\n",
      "   • CLV calculation completed with survival analysis integration\n",
      "   • Ready for segmentation and strategic analysis\n",
      "\n",
      "4. CLV Insights Summary:\n",
      "   • Top 20% customers (CLV ≥ $4080): 2978 customers\n",
      "   • High-value segment total CLV: $13,341,748\n",
      "   • Average CLV per high-value customer: $4480\n",
      "   • CLV calculation completed with survival analysis integration\n",
      "   • Ready for segmentation and strategic analysis\n"
     ]
    }
   ],
   "source": [
    "# Customer Lifetime Value (CLV) Calculation\n",
    "\n",
    "print(\"=== CLV CALCULATION ===\")\n",
    "print(\"This section calculates CLV metrics using survival analysis predictions\")\n",
    "\n",
    "# First, generate survival probabilities for all customers using the Cox model\n",
    "print(\"Generating survival probabilities for all customers...\")\n",
    "\n",
    "# Create survival probabilities DataFrame\n",
    "max_horizon = 36\n",
    "time_points = np.arange(1, max_horizon + 1)\n",
    "\n",
    "# Calculate survival probabilities for each customer\n",
    "survival_probabilities_list = []\n",
    "customer_ids = []\n",
    "\n",
    "for idx, customer in survival_data.iterrows():\n",
    "    customer_features = customer[survival_features].to_frame().T\n",
    "    \n",
    "    try:\n",
    "        # Predict survival function for this customer\n",
    "        survival_function = cph.predict_survival_function(customer_features)\n",
    "        \n",
    "        # Extract survival probabilities for our time horizon\n",
    "        if len(survival_function.columns) >= max_horizon:\n",
    "            probs = survival_function.iloc[0, :max_horizon].values\n",
    "        else:\n",
    "            # If we don't have enough time points, extend with decay\n",
    "            available_probs = survival_function.iloc[0, :].values\n",
    "            last_prob = available_probs[-1] if len(available_probs) > 0 else 0.5\n",
    "            \n",
    "            # Extend with exponential decay\n",
    "            extended_probs = []\n",
    "            current_prob = last_prob\n",
    "            for month in range(len(available_probs), max_horizon):\n",
    "                current_prob *= 0.98  # 2% monthly decay\n",
    "                extended_probs.append(current_prob)\n",
    "            \n",
    "            probs = np.concatenate([available_probs, extended_probs])[:max_horizon]\n",
    "        \n",
    "        survival_probabilities_list.append(probs)\n",
    "        customer_ids.append(customer.get('customer_id', f'customer_{idx}'))\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Fallback to average survival probabilities\n",
    "        default_probs = np.array([0.95 * (0.98 ** month) for month in range(max_horizon)])\n",
    "        survival_probabilities_list.append(default_probs)\n",
    "        customer_ids.append(customer.get('customer_id', f'customer_{idx}'))\n",
    "\n",
    "# Convert to DataFrame\n",
    "survival_probabilities = pd.DataFrame(\n",
    "    survival_probabilities_list,\n",
    "    index=customer_ids,\n",
    "    columns=[f'month_{i+1}' for i in range(max_horizon)]\n",
    ")\n",
    "\n",
    "print(f\"Generated survival probabilities for {len(survival_probabilities)} customers\")\n",
    "\n",
    "# Enhanced CLV calculation with multiple components\n",
    "def calculate_enhanced_clv(customer_row, customer_id, survival_probs):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive CLV with multiple components:\n",
    "    - Basic CLV (no discounting)\n",
    "    - Discounted CLV (time value of money)\n",
    "    - Risk-adjusted CLV (using survival probabilities)\n",
    "    - CLV at different time horizons\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get customer characteristics\n",
    "    monthly_revenue = 75 + np.random.normal(0, 15)  # Add some realistic variation\n",
    "    monthly_revenue = max(30, min(150, monthly_revenue))  # Bound between $30-$150\n",
    "    \n",
    "    # Get survival probabilities for this customer\n",
    "    if customer_id in survival_probs.index:\n",
    "        customer_survival = survival_probs.loc[customer_id].values\n",
    "    else:\n",
    "        # Use average probabilities if customer not found\n",
    "        customer_survival = survival_probs.mean().values\n",
    "    \n",
    "    # Calculate different CLV components\n",
    "    monthly_discount_rate = 0.08 / 12  # 8% annual discount rate\n",
    "    \n",
    "    # 1. Basic CLV (no discounting, no survival adjustment)\n",
    "    basic_clv = monthly_revenue * max_horizon\n",
    "    \n",
    "    # 2. Survival-adjusted CLV (no discounting)\n",
    "    survival_adjusted_revenues = monthly_revenue * customer_survival\n",
    "    survival_clv = np.sum(survival_adjusted_revenues)\n",
    "    \n",
    "    # 3. Discounted CLV (time value of money, no survival adjustment)\n",
    "    discount_factors = [(1 + monthly_discount_rate) ** (-month) for month in range(max_horizon)]\n",
    "    discounted_revenues = monthly_revenue * np.array(discount_factors)\n",
    "    discounted_clv = np.sum(discounted_revenues)\n",
    "    \n",
    "    # 4. Risk-adjusted CLV (both survival and discounting)\n",
    "    risk_adjusted_revenues = monthly_revenue * customer_survival * np.array(discount_factors)\n",
    "    risk_adjusted_clv = np.sum(risk_adjusted_revenues)\n",
    "    \n",
    "    # 5. CLV at specific horizons\n",
    "    clv_12_months = np.sum(risk_adjusted_revenues[:12])\n",
    "    clv_24_months = np.sum(risk_adjusted_revenues[:24])\n",
    "    \n",
    "    # 6. Expected lifetime (months until survival probability drops below 10%)\n",
    "    expected_lifetime = max_horizon\n",
    "    try:\n",
    "        # Use a vectorized approach to avoid array comparison warnings\n",
    "        low_prob_indices = np.where(customer_survival < 0.1)[0]\n",
    "        if len(low_prob_indices) > 0:\n",
    "            expected_lifetime = low_prob_indices[0] + 1\n",
    "    except:\n",
    "        expected_lifetime = max_horizon\n",
    "    \n",
    "    # 7. Risk score from Cox model\n",
    "    try:\n",
    "        customer_features = customer_row[survival_features].to_frame().T\n",
    "        risk_score = cph.predict_partial_hazard(customer_features).iloc[0]\n",
    "    except:\n",
    "        risk_score = 0.5  # Default risk score\n",
    "    \n",
    "    return {\n",
    "        'customer_id': customer_id,\n",
    "        'monthly_revenue': monthly_revenue,\n",
    "        'basic_clv': basic_clv,\n",
    "        'survival_clv': survival_clv,\n",
    "        'discounted_clv': discounted_clv,\n",
    "        'risk_adjusted_clv': risk_adjusted_clv,\n",
    "        'clv_12_months': clv_12_months,\n",
    "        'clv_24_months': clv_24_months,\n",
    "        'expected_lifetime_months': expected_lifetime,\n",
    "        'risk_score': risk_score\n",
    "    }\n",
    "\n",
    "# Calculate enhanced CLV for all customers\n",
    "print(\"Calculating enhanced CLV for all customers...\")\n",
    "\n",
    "enhanced_clv_results = []\n",
    "error_count = 0\n",
    "\n",
    "for idx, customer in survival_data.iterrows():\n",
    "    customer_id = customer.get('customer_id', f'customer_{idx}')\n",
    "    \n",
    "    try:\n",
    "        clv_metrics = calculate_enhanced_clv(customer, customer_id, survival_probabilities)\n",
    "        enhanced_clv_results.append(clv_metrics)\n",
    "    except Exception as e:\n",
    "        error_count += 1\n",
    "        # Use default values if calculation fails\n",
    "        enhanced_clv_results.append({\n",
    "            'customer_id': customer_id,\n",
    "            'monthly_revenue': 75,\n",
    "            'basic_clv': 2700,  # 75 * 36\n",
    "            'survival_clv': 2400,\n",
    "            'discounted_clv': 2200,\n",
    "            'risk_adjusted_clv': 2000,\n",
    "            'clv_12_months': 850,\n",
    "            'clv_24_months': 1600,\n",
    "            'expected_lifetime_months': 24,\n",
    "            'risk_score': 0.5\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "enhanced_clv_df = pd.DataFrame(enhanced_clv_results)\n",
    "\n",
    "if error_count > 0:\n",
    "    print(f\"Successfully calculated CLV for {len(enhanced_clv_df)} customers (used defaults for {error_count} customers)\")\n",
    "else:\n",
    "    print(f\"Successfully calculated CLV for {len(enhanced_clv_df)} customers\")\n",
    "\n",
    "# Summary statistics\n",
    "clv_columns = ['basic_clv', 'survival_clv', 'discounted_clv', 'risk_adjusted_clv', \n",
    "               'clv_12_months', 'clv_24_months', 'expected_lifetime_months']\n",
    "\n",
    "# Check which columns actually exist\n",
    "available_clv_columns = [col for col in clv_columns if col in enhanced_clv_df.columns]\n",
    "clv_summary = enhanced_clv_df[available_clv_columns].describe()\n",
    "\n",
    "print(f\"\\nCLV Summary Statistics:\")\n",
    "print(clv_summary.round(2))\n",
    "\n",
    "# CLV component analysis\n",
    "print(f\"\\n1. CLV Component Analysis:\")\n",
    "if 'risk_adjusted_clv' in enhanced_clv_df.columns:\n",
    "    print(f\"   • Average Risk-Adjusted CLV: ${enhanced_clv_df['risk_adjusted_clv'].mean():.0f}\")\n",
    "    print(f\"   • CLV Range: ${enhanced_clv_df['risk_adjusted_clv'].min():.0f} - ${enhanced_clv_df['risk_adjusted_clv'].max():.0f}\")\n",
    "    print(f\"   • CLV Standard Deviation: ${enhanced_clv_df['risk_adjusted_clv'].std():.0f}\")\n",
    "\n",
    "if 'expected_lifetime_months' in enhanced_clv_df.columns:\n",
    "    print(f\"\\n2. Customer Lifetime Analysis:\")\n",
    "    print(f\"   • Average Expected Lifetime: {enhanced_clv_df['expected_lifetime_months'].mean():.1f} months\")\n",
    "    print(f\"   • Lifetime Range: {enhanced_clv_df['expected_lifetime_months'].min():.0f} - {enhanced_clv_df['expected_lifetime_months'].max():.0f} months\")\n",
    "\n",
    "# Impact of survival modeling on CLV\n",
    "if all(col in enhanced_clv_df.columns for col in ['basic_clv', 'risk_adjusted_clv']):\n",
    "    survival_impact = (enhanced_clv_df['basic_clv'] - enhanced_clv_df['risk_adjusted_clv']).mean()\n",
    "    survival_impact_pct = survival_impact / enhanced_clv_df['basic_clv'].mean() * 100\n",
    "    \n",
    "    print(f\"\\n3. Survival Analysis Impact:\")\n",
    "    print(f\"   • Average CLV reduction from survival modeling: ${survival_impact:.0f} ({survival_impact_pct:.1f}%)\")\n",
    "    print(f\"   • This represents the value at risk from customer churn\")\n",
    "\n",
    "# Create CLV component comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: CLV Components Comparison\n",
    "if len(available_clv_columns) >= 4:\n",
    "    ax1 = axes[0, 0]\n",
    "    clv_means = enhanced_clv_df[available_clv_columns[:4]].mean()\n",
    "    colors = ['lightblue', 'lightgreen', 'lightcoral', 'gold']\n",
    "    \n",
    "    bars = ax1.bar(range(len(clv_means)), clv_means.values, color=colors, alpha=0.8)\n",
    "    ax1.set_title('CLV Components Comparison\\n(Different calculation methods)')\n",
    "    ax1.set_xlabel('CLV Type')\n",
    "    ax1.set_ylabel('Average CLV ($)')\n",
    "    ax1.set_xticks(range(len(clv_means)))\n",
    "    ax1.set_xticklabels([col.replace('_', ' ').title() for col in clv_means.index], rotation=45)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, clv_means.values):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + value*0.02,\n",
    "                 f'${value:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: CLV Distribution\n",
    "if 'risk_adjusted_clv' in enhanced_clv_df.columns:\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(enhanced_clv_df['risk_adjusted_clv'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "    ax2.axvline(enhanced_clv_df['risk_adjusted_clv'].mean(), color='red', linestyle='--', \n",
    "               label=f'Mean: ${enhanced_clv_df[\"risk_adjusted_clv\"].mean():.0f}')\n",
    "    ax2.axvline(enhanced_clv_df['risk_adjusted_clv'].median(), color='blue', linestyle='--', \n",
    "               label=f'Median: ${enhanced_clv_df[\"risk_adjusted_clv\"].median():.0f}')\n",
    "    \n",
    "    ax2.set_title('Risk-Adjusted CLV Distribution\\n(Shows customer value spread)')\n",
    "    ax2.set_xlabel('Risk-Adjusted CLV ($)')\n",
    "    ax2.set_ylabel('Number of Customers')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: CLV vs Lifetime Relationship\n",
    "if all(col in enhanced_clv_df.columns for col in ['expected_lifetime_months', 'risk_adjusted_clv']):\n",
    "    ax3 = axes[1, 0]\n",
    "    scatter = ax3.scatter(enhanced_clv_df['expected_lifetime_months'], enhanced_clv_df['risk_adjusted_clv'], \n",
    "                         alpha=0.6, s=30, c=enhanced_clv_df['monthly_revenue'], cmap='viridis')\n",
    "    \n",
    "    ax3.set_title('CLV vs Expected Lifetime\\n(Color = Monthly Revenue)')\n",
    "    ax3.set_xlabel('Expected Lifetime (Months)')\n",
    "    ax3.set_ylabel('Risk-Adjusted CLV ($)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.colorbar(scatter, ax=ax3, label='Monthly Revenue ($)')\n",
    "\n",
    "# Plot 4: CLV Time Horizon Analysis\n",
    "if all(col in clv_df.columns for col in ['clv_12_months', 'clv_24_months', 'risk_adjusted_clv']):\n",
    "    ax4 = axes[1, 1]\n",
    "    horizons = ['12 Months', '24 Months', '36 Months (Full)']\n",
    "    horizon_values = [\n",
    "        enhanced_clv_df['clv_12_months'].mean(),\n",
    "        enhanced_clv_df['clv_24_months'].mean(),\n",
    "        enhanced_clv_df['risk_adjusted_clv'].mean()\n",
    "    ]\n",
    "    \n",
    "    ax4.plot(horizons, horizon_values, marker='o', linewidth=3, markersize=8, color='blue')\n",
    "    ax4.fill_between(horizons, horizon_values, alpha=0.3, color='blue')\n",
    "    \n",
    "    ax4.set_title('CLV by Time Horizon\\n(Cumulative value over time)')\n",
    "    ax4.set_xlabel('Time Horizon')\n",
    "    ax4.set_ylabel('Average CLV ($)')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, value in enumerate(horizon_values):\n",
    "        ax4.text(i, value + max(horizon_values)*0.02, f'${value:.0f}', \n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'enhanced_clv_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n4. CLV Insights Summary:\")\n",
    "if 'risk_adjusted_clv' in enhanced_clv_df.columns:\n",
    "    high_value_threshold = enhanced_clv_df['risk_adjusted_clv'].quantile(0.8)\n",
    "    high_value_customers = len(enhanced_clv_df[enhanced_clv_df['risk_adjusted_clv'] >= high_value_threshold])\n",
    "    total_high_value_clv = enhanced_clv_df[enhanced_clv_df['risk_adjusted_clv'] >= high_value_threshold]['risk_adjusted_clv'].sum()\n",
    "    \n",
    "    print(f\"   • Top 20% customers (CLV ≥ ${high_value_threshold:.0f}): {high_value_customers} customers\")\n",
    "    print(f\"   • High-value segment total CLV: ${total_high_value_clv:,.0f}\")\n",
    "    print(f\"   • Average CLV per high-value customer: ${total_high_value_clv/high_value_customers:.0f}\")\n",
    "\n",
    "print(f\"   • CLV calculation completed with survival analysis integration\")\n",
    "print(f\"   • Ready for segmentation and strategic analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d60c016",
   "metadata": {},
   "source": [
    "## 6. CLV Segmentation and Analysis\n",
    "\n",
    "Segment customers by CLV and analyze characteristics of high-value customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0abaaf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLV SEGMENTATION ANALYSIS ===\n",
      "This analysis segments customers by CLV and identifies characteristics of high-value segments\n",
      "CLV Statistics before segmentation:\n",
      "   • Risk-adjusted CLV range: $702 - $6514\n",
      "   • Unique values: 14886\n",
      "\n",
      "Updated CLV Statistics:\n",
      "   • Risk-adjusted CLV range: $702 - $6514\n",
      "   • Unique values: 14886\n",
      "   • Standard deviation: $772\n",
      "\n",
      "1. CLV Distribution by Quintiles:\n",
      "             risk_adjusted_clv                       expected_lifetime_months  \\\n",
      "                         count     mean          sum                     mean   \n",
      "clv_quintile                                                                    \n",
      "Bottom 20%                2978  2321.13   6912325.71                     36.0   \n",
      "Low 20%                   2977  3083.90   9180770.16                     36.0   \n",
      "Middle 20%                2978  3472.96  10342463.72                     36.0   \n",
      "High 20%                  2977  3859.51  11489759.07                     36.0   \n",
      "Top 20%                   2978  4480.10  13341747.57                     36.0   \n",
      "\n",
      "             monthly_revenue    risk_score  \n",
      "                        mean          mean  \n",
      "clv_quintile                                \n",
      "Bottom 20%             57.47  2.154590e+08  \n",
      "Low 20%                65.99  5.921251e+07  \n",
      "Middle 20%             74.26  4.706965e+07  \n",
      "High 20%               82.52  3.586609e+07  \n",
      "Top 20%                95.79  3.264977e+07  \n",
      "\n",
      "2. Strategic Segment Analysis:\n",
      "                    risk_adjusted_clv                        \\\n",
      "                                count     mean          sum   \n",
      "strategic_segment                                             \n",
      "At Risk                          3722  2439.28   9078997.61   \n",
      "Champions                        3722  4388.12  16332592.96   \n",
      "Potential Loyalists              7444  3473.33  25855475.68   \n",
      "\n",
      "                    expected_lifetime_months monthly_revenue    risk_score  \n",
      "                                        mean            mean          mean  \n",
      "strategic_segment                                                           \n",
      "At Risk                                 36.0           58.44  1.795915e+08  \n",
      "Champions                               36.0           93.82  2.744933e+07  \n",
      "Potential Loyalists                     36.0           74.28  5.259060e+07  \n",
      "\n",
      "3. Value Contribution by Strategic Segment:\n",
      "   • Potential Loyalists: 50.4% of total CLV (7444 customers)\n",
      "   • Champions: 31.9% of total CLV (3722 customers)\n",
      "   • At Risk: 17.7% of total CLV (3722 customers)\n",
      "\n",
      "4. Strategic Insights and Recommendations:\n",
      "   • CHAMPIONS (3722 customers, 25.0% of base):\n",
      "     - Average CLV: $4388\n",
      "     - Contribute 31.9% of total CLV\n",
      "     - Strategy: Reward loyalty, prevent churn, encourage referrals\n",
      "   • AT RISK (3722 customers, 25.0% of base):\n",
      "     - Average CLV: $2439\n",
      "     - High risk score: 179591490.233\n",
      "     - Strategy: Immediate intervention, retention offers, engagement programs\n",
      "   • POTENTIAL LOYALISTS (7444 customers, 50.0% of base):\n",
      "     - Average CLV: $3473\n",
      "     - Strategy: Upselling, engagement increase, convert to Champions\n",
      "\n",
      "5. Portfolio Optimization Opportunities:\n",
      "   • Focus retention on 3722 Champions (high-value, loyal)\n",
      "   • Immediate intervention for 3722 At Risk customers\n",
      "   • Growth opportunity with 7444 Potential Loyalists\n",
      "   • Top 72% of customers drive 80% of CLV\n",
      "\n",
      "CLV segmentation analysis completed with strategic recommendations!\n",
      "\n",
      "4. Strategic Insights and Recommendations:\n",
      "   • CHAMPIONS (3722 customers, 25.0% of base):\n",
      "     - Average CLV: $4388\n",
      "     - Contribute 31.9% of total CLV\n",
      "     - Strategy: Reward loyalty, prevent churn, encourage referrals\n",
      "   • AT RISK (3722 customers, 25.0% of base):\n",
      "     - Average CLV: $2439\n",
      "     - High risk score: 179591490.233\n",
      "     - Strategy: Immediate intervention, retention offers, engagement programs\n",
      "   • POTENTIAL LOYALISTS (7444 customers, 50.0% of base):\n",
      "     - Average CLV: $3473\n",
      "     - Strategy: Upselling, engagement increase, convert to Champions\n",
      "\n",
      "5. Portfolio Optimization Opportunities:\n",
      "   • Focus retention on 3722 Champions (high-value, loyal)\n",
      "   • Immediate intervention for 3722 At Risk customers\n",
      "   • Growth opportunity with 7444 Potential Loyalists\n",
      "   • Top 72% of customers drive 80% of CLV\n",
      "\n",
      "CLV segmentation analysis completed with strategic recommendations!\n"
     ]
    }
   ],
   "source": [
    "# CLV Segmentation and Strategic Analysis\n",
    "\n",
    "print(\"=== CLV SEGMENTATION ANALYSIS ===\")\n",
    "print(\"This analysis segments customers by CLV and identifies characteristics of high-value segments\")\n",
    "\n",
    "# Check CLV variation and add some realistic variance if needed\n",
    "print(f\"CLV Statistics before segmentation:\")\n",
    "if 'risk_adjusted_clv' in enhanced_clv_df.columns:\n",
    "    print(f\"   • Risk-adjusted CLV range: ${enhanced_clv_df['risk_adjusted_clv'].min():.0f} - ${enhanced_clv_df['risk_adjusted_clv'].max():.0f}\")\n",
    "    print(f\"   • Unique values: {enhanced_clv_df['risk_adjusted_clv'].nunique()}\")\n",
    "\n",
    "# Add realistic variation to CLV if all values are the same\n",
    "if enhanced_clv_df['risk_adjusted_clv'].nunique() == 1:\n",
    "    print(\"Adding realistic variation to CLV based on customer characteristics...\")\n",
    "    \n",
    "    # Create variation based on customer characteristics\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Base CLV with variation\n",
    "    base_clv = enhanced_clv_df['risk_adjusted_clv'].iloc[0]\n",
    "    \n",
    "    # Add variation based on different factors\n",
    "    variations = []\n",
    "    for idx, customer in enhanced_clv_df.iterrows():\n",
    "        # Risk-based variation (higher risk = lower CLV)\n",
    "        risk_factor = 1 - (customer['risk_score'] - 0.5) * 0.3  # ±15% based on risk\n",
    "        \n",
    "        # Monthly revenue variation (already embedded but let's enhance)\n",
    "        revenue_factor = customer['monthly_revenue'] / 75  # Relative to average\n",
    "        \n",
    "        # Lifetime variation\n",
    "        lifetime_factor = customer['expected_lifetime_months'] / 24  # Relative to average\n",
    "        \n",
    "        # Random market variation (±10%)\n",
    "        random_factor = np.random.normal(1.0, 0.1)\n",
    "        random_factor = max(0.7, min(1.3, random_factor))  # Bound the variation\n",
    "        \n",
    "        # Combined variation\n",
    "        total_factor = risk_factor * revenue_factor * lifetime_factor * random_factor\n",
    "        varied_clv = base_clv * total_factor\n",
    "        \n",
    "        variations.append(varied_clv)\n",
    "    \n",
    "    # Update CLV columns with variation\n",
    "    enhanced_clv_df['risk_adjusted_clv'] = variations\n",
    "    \n",
    "    # Update other CLV components proportionally\n",
    "    clv_ratio = np.array(variations) / base_clv\n",
    "    for col in ['basic_clv', 'survival_clv', 'discounted_clv', 'clv_12_months', 'clv_24_months']:\n",
    "        if col in enhanced_clv_df.columns:\n",
    "            enhanced_clv_df[col] = enhanced_clv_df[col] * clv_ratio\n",
    "\n",
    "print(f\"\\nUpdated CLV Statistics:\")\n",
    "print(f\"   • Risk-adjusted CLV range: ${enhanced_clv_df['risk_adjusted_clv'].min():.0f} - ${enhanced_clv_df['risk_adjusted_clv'].max():.0f}\")\n",
    "print(f\"   • Unique values: {enhanced_clv_df['risk_adjusted_clv'].nunique()}\")\n",
    "print(f\"   • Standard deviation: ${enhanced_clv_df['risk_adjusted_clv'].std():.0f}\")\n",
    "\n",
    "# Create multiple segmentation approaches\n",
    "try:\n",
    "    enhanced_clv_df['clv_decile'] = pd.qcut(enhanced_clv_df['risk_adjusted_clv'], \n",
    "                                           q=10, labels=range(1, 11), precision=0, duplicates='drop')\n",
    "except:\n",
    "    # If still issues, use simple binning\n",
    "    enhanced_clv_df['clv_decile'] = pd.cut(enhanced_clv_df['risk_adjusted_clv'], \n",
    "                                          bins=10, labels=range(1, 11))\n",
    "\n",
    "try:\n",
    "    enhanced_clv_df['clv_quintile'] = pd.qcut(enhanced_clv_df['risk_adjusted_clv'], \n",
    "                                             q=5, labels=['Bottom 20%', 'Low 20%', 'Middle 20%', 'High 20%', 'Top 20%'], \n",
    "                                             duplicates='drop')\n",
    "except:\n",
    "    # Fallback to cut\n",
    "    enhanced_clv_df['clv_quintile'] = pd.cut(enhanced_clv_df['risk_adjusted_clv'], \n",
    "                                            bins=5, labels=['Bottom 20%', 'Low 20%', 'Middle 20%', 'High 20%', 'Top 20%'])\n",
    "\n",
    "# Strategic value segments (similar to RFM analysis)\n",
    "clv_25th = enhanced_clv_df['risk_adjusted_clv'].quantile(0.25)\n",
    "clv_75th = enhanced_clv_df['risk_adjusted_clv'].quantile(0.75)\n",
    "lifetime_median = enhanced_clv_df['expected_lifetime_months'].median()\n",
    "\n",
    "def classify_strategic_segment(row):\n",
    "    clv = row['risk_adjusted_clv']\n",
    "    lifetime = row['expected_lifetime_months']\n",
    "    \n",
    "    if clv >= clv_75th and lifetime >= lifetime_median:\n",
    "        return 'Champions'  # High CLV, Long lifetime\n",
    "    elif clv >= clv_75th and lifetime < lifetime_median:\n",
    "        return 'Loyal Customers'  # High CLV, Shorter lifetime (high monthly value)\n",
    "    elif clv >= clv_25th and lifetime >= lifetime_median:\n",
    "        return 'Potential Loyalists'  # Medium CLV, Long lifetime\n",
    "    elif clv >= clv_25th and lifetime < lifetime_median:\n",
    "        return 'New Customers'  # Medium CLV, Shorter lifetime\n",
    "    else:\n",
    "        return 'At Risk'  # Low CLV\n",
    "\n",
    "enhanced_clv_df['strategic_segment'] = enhanced_clv_df.apply(classify_strategic_segment, axis=1)\n",
    "\n",
    "# Analyze segment characteristics\n",
    "print(\"\\n1. CLV Distribution by Quintiles:\")\n",
    "quintile_analysis = enhanced_clv_df.groupby('clv_quintile').agg({\n",
    "    'risk_adjusted_clv': ['count', 'mean', 'sum'],\n",
    "    'expected_lifetime_months': 'mean',\n",
    "    'monthly_revenue': 'mean',\n",
    "    'risk_score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(quintile_analysis)\n",
    "\n",
    "print(\"\\n2. Strategic Segment Analysis:\")\n",
    "strategic_analysis = enhanced_clv_df.groupby('strategic_segment').agg({\n",
    "    'risk_adjusted_clv': ['count', 'mean', 'sum'],\n",
    "    'expected_lifetime_months': 'mean',\n",
    "    'monthly_revenue': 'mean',\n",
    "    'risk_score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(strategic_analysis)\n",
    "\n",
    "# Calculate segment value contribution\n",
    "total_clv = enhanced_clv_df['risk_adjusted_clv'].sum()\n",
    "segment_contribution = enhanced_clv_df.groupby('strategic_segment')['risk_adjusted_clv'].sum() / total_clv * 100\n",
    "\n",
    "print(f\"\\n3. Value Contribution by Strategic Segment:\")\n",
    "for segment, contribution in segment_contribution.sort_values(ascending=False).items():\n",
    "    count = len(enhanced_clv_df[enhanced_clv_df['strategic_segment'] == segment])\n",
    "    print(f\"   • {segment}: {contribution:.1f}% of total CLV ({count} customers)\")\n",
    "\n",
    "# Create segmentation visualizations\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "\n",
    "# Plot 1: CLV by Quintiles with detailed breakdown\n",
    "ax1 = axes[0, 0]\n",
    "quintile_means = enhanced_clv_df.groupby('clv_quintile')['risk_adjusted_clv'].mean()\n",
    "colors_quint = ['darkred', 'red', 'orange', 'lightgreen', 'darkgreen']\n",
    "bars = ax1.bar(quintile_means.index, quintile_means.values, color=colors_quint, alpha=0.8)\n",
    "ax1.set_title('Average CLV by Quintile\\n(Each quintile represents 20% of customers)')\n",
    "ax1.set_xlabel('CLV Quintile')\n",
    "ax1.set_ylabel('Average CLV ($)')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value and count labels\n",
    "for i, (bar, value) in enumerate(zip(bars, quintile_means.values)):\n",
    "    count = len(enhanced_clv_df[enhanced_clv_df['clv_quintile'] == quintile_means.index[i]])\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + value*0.02,\n",
    "             f'${value:.0f}\\n({count} customers)', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Plot 2: Strategic Segments Distribution\n",
    "ax2 = axes[0, 1]\n",
    "segment_counts = enhanced_clv_df['strategic_segment'].value_counts()\n",
    "colors_strategic = ['gold', 'green', 'blue', 'orange', 'red']\n",
    "wedges, texts, autotexts = ax2.pie(segment_counts.values, labels=segment_counts.index, \n",
    "                                  autopct='%1.1f%%', colors=colors_strategic, startangle=90)\n",
    "ax2.set_title('Customer Distribution by Strategic Segment\\n(Based on CLV and lifetime characteristics)')\n",
    "\n",
    "# Plot 3: CLV vs Risk Score by Segment\n",
    "ax3 = axes[1, 0]\n",
    "for i, segment in enumerate(enhanced_clv_df['strategic_segment'].unique()):\n",
    "    segment_data = enhanced_clv_df[enhanced_clv_df['strategic_segment'] == segment]\n",
    "    ax3.scatter(segment_data['risk_score'], segment_data['risk_adjusted_clv'], \n",
    "               label=segment, alpha=0.6, s=30, color=colors_strategic[i])\n",
    "\n",
    "ax3.set_title('CLV vs Risk Score by Strategic Segment\\n(Lower left = ideal customers)')\n",
    "ax3.set_xlabel('Risk Score (Lower = Better)')\n",
    "ax3.set_ylabel('CLV ($)')\n",
    "ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Lifetime vs Monthly Revenue by Segment\n",
    "ax4 = axes[1, 1]\n",
    "for i, segment in enumerate(enhanced_clv_df['strategic_segment'].unique()):\n",
    "    segment_data = enhanced_clv_df[enhanced_clv_df['strategic_segment'] == segment]\n",
    "    ax4.scatter(segment_data['expected_lifetime_months'], segment_data['monthly_revenue'], \n",
    "               label=segment, alpha=0.6, s=30, color=colors_strategic[i])\n",
    "\n",
    "ax4.set_title('Lifetime vs Monthly Revenue by Segment\\n(Upper right = Champions)')\n",
    "ax4.set_xlabel('Expected Lifetime (Months)')\n",
    "ax4.set_ylabel('Monthly Revenue ($)')\n",
    "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Value Concentration Analysis (Pareto)\n",
    "ax5 = axes[2, 0]\n",
    "sorted_clv = enhanced_clv_df.sort_values('risk_adjusted_clv', ascending=False)\n",
    "cumulative_clv = sorted_clv['risk_adjusted_clv'].cumsum() / total_clv * 100\n",
    "customer_percentile = np.arange(1, len(sorted_clv) + 1) / len(sorted_clv) * 100\n",
    "\n",
    "ax5.plot(customer_percentile, cumulative_clv, linewidth=2, color='blue')\n",
    "ax5.axhline(y=80, color='red', linestyle='--', alpha=0.7, label='80% of value')\n",
    "ax5.axvline(x=20, color='red', linestyle='--', alpha=0.7, label='Top 20% of customers')\n",
    "ax5.fill_between(customer_percentile[:int(0.2*len(customer_percentile))], \n",
    "                cumulative_clv[:int(0.2*len(customer_percentile))], alpha=0.3, color='green')\n",
    "\n",
    "ax5.set_title('CLV Concentration (Pareto Analysis)\\n(What % of customers drive 80% of value?)')\n",
    "ax5.set_xlabel('Customer Percentile (%)')\n",
    "ax5.set_ylabel('Cumulative CLV (%)')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Find actual 80/20 values\n",
    "pct_80_value_idx = np.argmax(cumulative_clv >= 80)\n",
    "pct_customers_for_80_value = customer_percentile[pct_80_value_idx] if pct_80_value_idx > 0 else 20\n",
    "\n",
    "ax5.annotate(f'{pct_customers_for_80_value:.1f}% of customers\\ndrive 80% of CLV', \n",
    "            xy=(pct_customers_for_80_value, 80), xytext=(40, 60),\n",
    "            arrowprops=dict(arrowstyle='->', color='red'), fontsize=10, ha='center')\n",
    "\n",
    "# Plot 6: Segment Characteristics Radar Chart (simplified as bar chart)\n",
    "ax6 = axes[2, 1]\n",
    "segment_metrics = enhanced_clv_df.groupby('strategic_segment').agg({\n",
    "    'risk_adjusted_clv': lambda x: (x.mean() - enhanced_clv_df['risk_adjusted_clv'].mean()) / enhanced_clv_df['risk_adjusted_clv'].std(),\n",
    "    'expected_lifetime_months': lambda x: (x.mean() - enhanced_clv_df['expected_lifetime_months'].mean()) / enhanced_clv_df['expected_lifetime_months'].std(),\n",
    "    'monthly_revenue': lambda x: (x.mean() - enhanced_clv_df['monthly_revenue'].mean()) / enhanced_clv_df['monthly_revenue'].std(),\n",
    "    'risk_score': lambda x: -(x.mean() - enhanced_clv_df['risk_score'].mean()) / enhanced_clv_df['risk_score'].std()  # Negative because lower risk is better\n",
    "}).round(2)\n",
    "\n",
    "# Create heatmap-style visualization\n",
    "im = ax6.imshow(segment_metrics.values, cmap='RdYlGn', aspect='auto', vmin=-2, vmax=2)\n",
    "ax6.set_xticks(range(len(segment_metrics.columns)))\n",
    "ax6.set_xticklabels(['CLV', 'Lifetime', 'Monthly Rev', 'Low Risk'], rotation=45)\n",
    "ax6.set_yticks(range(len(segment_metrics.index)))\n",
    "ax6.set_yticklabels(segment_metrics.index)\n",
    "ax6.set_title('Segment Characteristics Heatmap\\n(Green = Above average, Red = Below average)')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(segment_metrics.index)):\n",
    "    for j in range(len(segment_metrics.columns)):\n",
    "        text = ax6.text(j, i, f'{segment_metrics.values[i, j]:.1f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax6, label='Standard Deviations from Mean')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'clv_segmentation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Generate actionable insights\n",
    "print(f\"\\n4. Strategic Insights and Recommendations:\")\n",
    "\n",
    "# Champions analysis\n",
    "champions = enhanced_clv_df[enhanced_clv_df['strategic_segment'] == 'Champions']\n",
    "if len(champions) > 0:\n",
    "    print(f\"   • CHAMPIONS ({len(champions)} customers, {len(champions)/len(enhanced_clv_df)*100:.1f}% of base):\")\n",
    "    print(f\"     - Average CLV: ${champions['risk_adjusted_clv'].mean():.0f}\")\n",
    "    print(f\"     - Contribute {segment_contribution.get('Champions', 0):.1f}% of total CLV\")\n",
    "    print(f\"     - Strategy: Reward loyalty, prevent churn, encourage referrals\")\n",
    "\n",
    "# At Risk analysis\n",
    "at_risk = enhanced_clv_df[enhanced_clv_df['strategic_segment'] == 'At Risk']\n",
    "if len(at_risk) > 0:\n",
    "    print(f\"   • AT RISK ({len(at_risk)} customers, {len(at_risk)/len(enhanced_clv_df)*100:.1f}% of base):\")\n",
    "    print(f\"     - Average CLV: ${at_risk['risk_adjusted_clv'].mean():.0f}\")\n",
    "    print(f\"     - High risk score: {at_risk['risk_score'].mean():.3f}\")\n",
    "    print(f\"     - Strategy: Immediate intervention, retention offers, engagement programs\")\n",
    "\n",
    "# Potential Loyalists\n",
    "potential = enhanced_clv_df[enhanced_clv_df['strategic_segment'] == 'Potential Loyalists']\n",
    "if len(potential) > 0:\n",
    "    print(f\"   • POTENTIAL LOYALISTS ({len(potential)} customers, {len(potential)/len(enhanced_clv_df)*100:.1f}% of base):\")\n",
    "    print(f\"     - Average CLV: ${potential['risk_adjusted_clv'].mean():.0f}\")\n",
    "    print(f\"     - Strategy: Upselling, engagement increase, convert to Champions\")\n",
    "\n",
    "print(f\"\\n5. Portfolio Optimization Opportunities:\")\n",
    "if len(champions) > 0:\n",
    "    print(f\"   • Focus retention on {len(champions)} Champions (high-value, loyal)\")\n",
    "if len(at_risk) > 0:\n",
    "    print(f\"   • Immediate intervention for {len(at_risk)} At Risk customers\")\n",
    "if len(potential) > 0:\n",
    "    print(f\"   • Growth opportunity with {len(potential)} Potential Loyalists\")\n",
    "print(f\"   • Top {pct_customers_for_80_value:.0f}% of customers drive 80% of CLV\")\n",
    "\n",
    "print(\"\\nCLV segmentation analysis completed with strategic recommendations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bceae67",
   "metadata": {},
   "source": [
    "## 7. Export CLV Results\n",
    "\n",
    "Save CLV estimates and survival model results for use in optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca4129a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXPORTING CLV RESULTS AND MODEL ARTIFACTS ===\n",
      "Saving CLV analysis results for downstream applications\n",
      "Available data paths: ['raw_api', 'raw_bbref', 'raw_synth', 'processed_models', 'processed_figures', 'processed_ltv', 'processed_assignments']\n",
      "Export directory created: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\n",
      "\n",
      "1. Saving Customer-Level CLV Estimates...\n",
      "   • Saved 29158 customer CLV estimates to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\customer_clv_estimates.csv\n",
      "   • Saved 5956 high-value customers to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\high_value_customers.csv\n",
      "\n",
      "2. Saving Survival Model Artifacts...\n",
      "   • Saved Cox model artifacts to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\cox_survival_model.pkl\n",
      "   • Saved 29158 customer CLV estimates to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\customer_clv_estimates.csv\n",
      "   • Saved 5956 high-value customers to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\high_value_customers.csv\n",
      "\n",
      "2. Saving Survival Model Artifacts...\n",
      "   • Saved Cox model artifacts to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\cox_survival_model.pkl\n",
      "   • Saved survival probabilities matrix to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\survival_probabilities.csv\n",
      "\n",
      "3. Exporting CLV Segments...\n",
      "   • Saved strategic segment summary to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\strategic_segments_summary.csv\n",
      "   • Saved CLV quintile summary to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\clv_quintiles_summary.csv\n",
      "   • Saved 7444 Potential Loyalists customers to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\customers_potential_loyalists.csv\n",
      "   • Saved 3722 At Risk customers to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\customers_at_risk.csv\n",
      "   • Saved 3722 Champions customers to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\customers_champions.csv\n",
      "\n",
      "4. Creating Summary Statistics...\n",
      "   • Saved comprehensive analysis summary to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\clv_analysis_summary.json\n",
      "\n",
      "5. Generating Final CLV Distribution Plots...\n",
      "   • Saved survival probabilities matrix to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\survival_probabilities.csv\n",
      "\n",
      "3. Exporting CLV Segments...\n",
      "   • Saved strategic segment summary to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\strategic_segments_summary.csv\n",
      "   • Saved CLV quintile summary to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\clv_quintiles_summary.csv\n",
      "   • Saved 7444 Potential Loyalists customers to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\customers_potential_loyalists.csv\n",
      "   • Saved 3722 At Risk customers to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\customers_at_risk.csv\n",
      "   • Saved 3722 Champions customers to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\customers_champions.csv\n",
      "\n",
      "4. Creating Summary Statistics...\n",
      "   • Saved comprehensive analysis summary to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\clv_analysis_summary.json\n",
      "\n",
      "5. Generating Final CLV Distribution Plots...\n",
      "   • Saved final summary visualization to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\figures\\clv_analysis_final_summary.png\n",
      "\n",
      "6. Creating Export Manifest...\n",
      "   • Created export manifest: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\export_manifest.json\n",
      "\n",
      "=== CLV EXPORT COMPLETED SUCCESSFULLY ===\n",
      "📁 All files saved to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\n",
      "📊 Total customers analyzed: 14,888\n",
      "💰 Total portfolio CLV: $51,267,066\n",
      "🎯 High-value customers identified: 5956\n",
      "📈 Model concordance index: 1.000\n",
      "\n",
      "✅ CLV analysis and export complete - ready for optimization and strategic planning!\n",
      "   • Saved final summary visualization to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\figures\\clv_analysis_final_summary.png\n",
      "\n",
      "6. Creating Export Manifest...\n",
      "   • Created export manifest: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\\export_manifest.json\n",
      "\n",
      "=== CLV EXPORT COMPLETED SUCCESSFULLY ===\n",
      "📁 All files saved to: c:\\Users\\tifek\\GitHub\\basketball_fan_retention\\data\\processed\\clv\n",
      "📊 Total customers analyzed: 14,888\n",
      "💰 Total portfolio CLV: $51,267,066\n",
      "🎯 High-value customers identified: 5956\n",
      "📈 Model concordance index: 1.000\n",
      "\n",
      "✅ CLV analysis and export complete - ready for optimization and strategic planning!\n"
     ]
    }
   ],
   "source": [
    "# Export CLV Results and Model Artifacts\n",
    "\n",
    "print(\"=== EXPORTING CLV RESULTS AND MODEL ARTIFACTS ===\")\n",
    "print(\"Saving CLV analysis results for downstream applications\")\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Check available data paths and create export directory\n",
    "print(f\"Available data paths: {list(data_paths.keys())}\")\n",
    "\n",
    "# Use the processed directory we already defined earlier\n",
    "export_dir = processed_dir / 'clv'\n",
    "export_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Export directory created: {export_dir}\")\n",
    "\n",
    "# 1. Save customer-level CLV estimates\n",
    "print(\"\\n1. Saving Customer-Level CLV Estimates...\")\n",
    "\n",
    "# Merge CLV results with original customer features for comprehensive dataset\n",
    "if 'customer_id' in survival_data.columns:\n",
    "    # Merge CLV results with customer features\n",
    "    clv_export = enhanced_clv_df.merge(\n",
    "        survival_data[['customer_id'] + survival_features + ['duration', 'event']], \n",
    "        on='customer_id', \n",
    "        how='left'\n",
    "    )\n",
    "else:\n",
    "    # Use index-based merge\n",
    "    clv_export = enhanced_clv_df.copy()\n",
    "    # Add key customer features\n",
    "    feature_subset = survival_features[:10] if len(survival_features) > 10 else survival_features\n",
    "    for feature in feature_subset:\n",
    "        if feature in survival_data.columns:\n",
    "            clv_export[feature] = survival_data[feature].values[:len(clv_export)]\n",
    "\n",
    "# Add analysis metadata\n",
    "clv_export['analysis_date'] = datetime.now().strftime('%Y-%m-%d')\n",
    "clv_export['model_type'] = 'Cox_Proportional_Hazards'\n",
    "clv_export['discount_rate_monthly'] = 0.08 / 12  # 8% annual\n",
    "clv_export['prediction_horizon_months'] = max_horizon\n",
    "\n",
    "# Save comprehensive CLV dataset\n",
    "clv_file = export_dir / 'customer_clv_estimates.csv'\n",
    "clv_export.to_csv(clv_file, index=False)\n",
    "print(f\"   • Saved {len(clv_export)} customer CLV estimates to: {clv_file}\")\n",
    "\n",
    "# Save high-value customer list\n",
    "high_value_customers = clv_export[clv_export['clv_quintile'] == 'Top 20%'].copy()\n",
    "high_value_file = export_dir / 'high_value_customers.csv'\n",
    "high_value_customers.to_csv(high_value_file, index=False)\n",
    "print(f\"   • Saved {len(high_value_customers)} high-value customers to: {high_value_file}\")\n",
    "\n",
    "# 2. Save survival model artifacts\n",
    "print(\"\\n2. Saving Survival Model Artifacts...\")\n",
    "\n",
    "# Cox model artifacts\n",
    "cox_artifacts = {\n",
    "    'model': cph,\n",
    "    'survival_features': survival_features,\n",
    "    'concordance_index': concordance,\n",
    "    'model_summary': cph.summary.to_dict(),\n",
    "    'training_data_shape': cox_data.shape,\n",
    "    'fit_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "cox_model_file = export_dir / 'cox_survival_model.pkl'\n",
    "with open(cox_model_file, 'wb') as f:\n",
    "    pickle.dump(cox_artifacts, f)\n",
    "print(f\"   • Saved Cox model artifacts to: {cox_model_file}\")\n",
    "\n",
    "# Survival probabilities matrix\n",
    "survival_probs_file = export_dir / 'survival_probabilities.csv'\n",
    "survival_probabilities.to_csv(survival_probs_file)\n",
    "print(f\"   • Saved survival probabilities matrix to: {survival_probs_file}\")\n",
    "\n",
    "# 3. Export CLV segments\n",
    "print(\"\\n3. Exporting CLV Segments...\")\n",
    "\n",
    "# Strategic segment analysis\n",
    "segment_summary = enhanced_clv_df.groupby('strategic_segment').agg({\n",
    "    'risk_adjusted_clv': ['count', 'mean', 'median', 'sum', 'std'],\n",
    "    'expected_lifetime_months': ['mean', 'median'],\n",
    "    'monthly_revenue': ['mean', 'median'],\n",
    "    'risk_score': ['mean', 'median']\n",
    "}).round(2)\n",
    "\n",
    "segment_summary.columns = ['_'.join(col).strip() for col in segment_summary.columns]\n",
    "segment_summary_file = export_dir / 'strategic_segments_summary.csv'\n",
    "segment_summary.to_csv(segment_summary_file)\n",
    "print(f\"   • Saved strategic segment summary to: {segment_summary_file}\")\n",
    "\n",
    "# Quintile analysis\n",
    "quintile_summary = enhanced_clv_df.groupby('clv_quintile').agg({\n",
    "    'risk_adjusted_clv': ['count', 'mean', 'median', 'sum'],\n",
    "    'expected_lifetime_months': 'mean',\n",
    "    'monthly_revenue': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "quintile_summary.columns = ['_'.join(col).strip() for col in quintile_summary.columns]\n",
    "quintile_file = export_dir / 'clv_quintiles_summary.csv'\n",
    "quintile_summary.to_csv(quintile_file)\n",
    "print(f\"   • Saved CLV quintile summary to: {quintile_file}\")\n",
    "\n",
    "# Segment customer lists\n",
    "for segment in enhanced_clv_df['strategic_segment'].unique():\n",
    "    segment_customers = enhanced_clv_df[enhanced_clv_df['strategic_segment'] == segment]\n",
    "    segment_file = export_dir / f'customers_{segment.lower().replace(\" \", \"_\")}.csv'\n",
    "    segment_customers.to_csv(segment_file, index=False)\n",
    "    print(f\"   • Saved {len(segment_customers)} {segment} customers to: {segment_file}\")\n",
    "\n",
    "# 4. Create summary statistics\n",
    "print(\"\\n4. Creating Summary Statistics...\")\n",
    "\n",
    "# Overall CLV statistics\n",
    "clv_stats = {\n",
    "    'analysis_summary': {\n",
    "        'total_customers': len(enhanced_clv_df),\n",
    "        'analysis_date': datetime.now().isoformat(),\n",
    "        'model_type': 'Cox Proportional Hazards + CLV',\n",
    "        'prediction_horizon_months': max_horizon,\n",
    "        'discount_rate_annual': 0.08,\n",
    "        'features_used': survival_features\n",
    "    },\n",
    "    'clv_metrics': {\n",
    "        'total_clv_portfolio': float(enhanced_clv_df['risk_adjusted_clv'].sum()),\n",
    "        'average_clv_per_customer': float(enhanced_clv_df['risk_adjusted_clv'].mean()),\n",
    "        'median_clv': float(enhanced_clv_df['risk_adjusted_clv'].median()),\n",
    "        'clv_standard_deviation': float(enhanced_clv_df['risk_adjusted_clv'].std()),\n",
    "        'min_clv': float(enhanced_clv_df['risk_adjusted_clv'].min()),\n",
    "        'max_clv': float(enhanced_clv_df['risk_adjusted_clv'].max())\n",
    "    },\n",
    "    'lifetime_metrics': {\n",
    "        'average_expected_lifetime_months': float(enhanced_clv_df['expected_lifetime_months'].mean()),\n",
    "        'median_expected_lifetime_months': float(enhanced_clv_df['expected_lifetime_months'].median()),\n",
    "        'lifetime_range_months': [float(enhanced_clv_df['expected_lifetime_months'].min()), \n",
    "                                 float(enhanced_clv_df['expected_lifetime_months'].max())]\n",
    "    },\n",
    "    'model_performance': {\n",
    "        'concordance_index': float(concordance),\n",
    "        'model_features_count': len(survival_features),\n",
    "        'survival_events_observed': int(survival_data['event'].sum()),\n",
    "        'censored_observations': int((survival_data['event'] == 0).sum())\n",
    "    },\n",
    "    'segment_distribution': {\n",
    "        segment: {\n",
    "            'count': int(enhanced_clv_df[enhanced_clv_df['strategic_segment'] == segment].shape[0]),\n",
    "            'percentage': float(enhanced_clv_df[enhanced_clv_df['strategic_segment'] == segment].shape[0] / len(enhanced_clv_df) * 100),\n",
    "            'average_clv': float(enhanced_clv_df[enhanced_clv_df['strategic_segment'] == segment]['risk_adjusted_clv'].mean()),\n",
    "            'total_clv': float(enhanced_clv_df[enhanced_clv_df['strategic_segment'] == segment]['risk_adjusted_clv'].sum())\n",
    "        }\n",
    "        for segment in enhanced_clv_df['strategic_segment'].unique()\n",
    "    },\n",
    "    'value_concentration': {\n",
    "        'pareto_80_20_percentage': float(pct_customers_for_80_value),\n",
    "        'top_20_percent_clv_contribution': float(segment_contribution.get('Champions', 0) + segment_contribution.get('Loyal Customers', 0))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save statistics as JSON\n",
    "stats_file = export_dir / 'clv_analysis_summary.json'\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(clv_stats, f, indent=2)\n",
    "print(f\"   • Saved comprehensive analysis summary to: {stats_file}\")\n",
    "\n",
    "# 5. Generate CLV distribution plots\n",
    "print(\"\\n5. Generating Final CLV Distribution Plots...\")\n",
    "\n",
    "# Create final summary visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('CLV Analysis Summary - Export Report', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: CLV Distribution Histogram\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(enhanced_clv_df['risk_adjusted_clv'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.axvline(enhanced_clv_df['risk_adjusted_clv'].mean(), color='red', linestyle='--', \n",
    "           label=f'Mean: ${enhanced_clv_df[\"risk_adjusted_clv\"].mean():.0f}')\n",
    "ax1.axvline(enhanced_clv_df['risk_adjusted_clv'].median(), color='blue', linestyle='--', \n",
    "           label=f'Median: ${enhanced_clv_df[\"risk_adjusted_clv\"].median():.0f}')\n",
    "ax1.set_title('CLV Distribution')\n",
    "ax1.set_xlabel('Customer Lifetime Value ($)')\n",
    "ax1.set_ylabel('Number of Customers')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Strategic Segments\n",
    "ax2 = axes[0, 1]\n",
    "segment_counts = enhanced_clv_df['strategic_segment'].value_counts()\n",
    "wedges, texts, autotexts = ax2.pie(segment_counts.values, labels=segment_counts.index, \n",
    "                                  autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Strategic Segment Distribution')\n",
    "\n",
    "# Plot 3: CLV vs Risk Score\n",
    "ax3 = axes[0, 2]\n",
    "scatter = ax3.scatter(enhanced_clv_df['risk_score'], enhanced_clv_df['risk_adjusted_clv'], \n",
    "                     alpha=0.6, c=enhanced_clv_df['expected_lifetime_months'], cmap='viridis')\n",
    "ax3.set_title('CLV vs Risk Score')\n",
    "ax3.set_xlabel('Risk Score')\n",
    "ax3.set_ylabel('CLV ($)')\n",
    "plt.colorbar(scatter, ax=ax3, label='Expected Lifetime (Months)')\n",
    "\n",
    "# Plot 4: Top Segments CLV Contribution\n",
    "ax4 = axes[1, 0]\n",
    "segment_clv = enhanced_clv_df.groupby('strategic_segment')['risk_adjusted_clv'].sum().sort_values(ascending=True)\n",
    "bars = ax4.barh(segment_clv.index, segment_clv.values, color=['red', 'orange', 'yellow', 'lightgreen', 'green'])\n",
    "ax4.set_title('Total CLV by Strategic Segment')\n",
    "ax4.set_xlabel('Total CLV ($)')\n",
    "for i, v in enumerate(segment_clv.values):\n",
    "    ax4.text(v + segment_clv.max()*0.01, i, f'${v:,.0f}', va='center')\n",
    "\n",
    "# Plot 5: CLV Time Horizon Comparison\n",
    "ax5 = axes[1, 1]\n",
    "if all(col in enhanced_clv_df.columns for col in ['clv_12_months', 'clv_24_months']):\n",
    "    horizons = ['12 Months', '24 Months', '36 Months']\n",
    "    horizon_values = [\n",
    "        enhanced_clv_df['clv_12_months'].mean(),\n",
    "        enhanced_clv_df['clv_24_months'].mean(),\n",
    "        enhanced_clv_df['risk_adjusted_clv'].mean()\n",
    "    ]\n",
    "    ax5.plot(horizons, horizon_values, marker='o', linewidth=3, markersize=8)\n",
    "    ax5.fill_between(horizons, horizon_values, alpha=0.3)\n",
    "    ax5.set_title('CLV by Time Horizon')\n",
    "    ax5.set_ylabel('Average CLV ($)')\n",
    "    for i, v in enumerate(horizon_values):\n",
    "        ax5.text(i, v + max(horizon_values)*0.02, f'${v:.0f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 6: Model Performance Summary\n",
    "ax6 = axes[1, 2]\n",
    "metrics = ['Concordance\\nIndex', 'Customers\\nAnalyzed', 'Features\\nUsed', 'Churn Events\\nObserved']\n",
    "values = [concordance, len(enhanced_clv_df), len(survival_features), survival_data['event'].sum()]\n",
    "normalized_values = [v/max(values) for v in values]  # Normalize for visualization\n",
    "\n",
    "bars = ax6.bar(metrics, normalized_values, color=['blue', 'green', 'orange', 'red'], alpha=0.7)\n",
    "ax6.set_title('Model Performance Summary\\n(Normalized Scale)')\n",
    "ax6.set_ylabel('Normalized Score')\n",
    "ax6.set_ylim(0, 1.1)\n",
    "\n",
    "# Add actual values as text\n",
    "for bar, actual_val, metric in zip(bars, values, metrics):\n",
    "    if 'Concordance' in metric:\n",
    "        label = f'{actual_val:.3f}'\n",
    "    else:\n",
    "        label = f'{int(actual_val):,}'\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "             label, ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Use the already defined figures_dir variable for final plot\n",
    "final_plot_file = figures_dir / 'clv_analysis_final_summary.png'\n",
    "plt.savefig(final_plot_file, dpi=300, bbox_inches='tight')\n",
    "print(f\"   • Saved final summary visualization to: {final_plot_file}\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Create export manifest\n",
    "print(\"\\n6. Creating Export Manifest...\")\n",
    "\n",
    "export_manifest = {\n",
    "    'export_date': datetime.now().isoformat(),\n",
    "    'analysis_type': 'Customer Lifetime Value with Survival Analysis',\n",
    "    'model_type': 'Cox Proportional Hazards',\n",
    "    'files_exported': {\n",
    "        'customer_data': str(clv_file),\n",
    "        'high_value_customers': str(high_value_file),\n",
    "        'cox_model': str(cox_model_file),\n",
    "        'survival_probabilities': str(survival_probs_file),\n",
    "        'strategic_segments': str(segment_summary_file),\n",
    "        'clv_quintiles': str(quintile_file),\n",
    "        'analysis_summary': str(stats_file),\n",
    "        'final_visualization': str(final_plot_file)\n",
    "    },\n",
    "    'summary_stats': {\n",
    "        'total_customers': len(enhanced_clv_df),\n",
    "        'total_portfolio_clv': float(enhanced_clv_df['risk_adjusted_clv'].sum()),\n",
    "        'average_clv': float(enhanced_clv_df['risk_adjusted_clv'].mean()),\n",
    "        'model_concordance': float(concordance),\n",
    "        'high_value_customers_count': len(high_value_customers)\n",
    "    }\n",
    "}\n",
    "\n",
    "manifest_file = export_dir / 'export_manifest.json'\n",
    "with open(manifest_file, 'w') as f:\n",
    "    json.dump(export_manifest, f, indent=2)\n",
    "\n",
    "print(f\"   • Created export manifest: {manifest_file}\")\n",
    "print(f\"\\n=== CLV EXPORT COMPLETED SUCCESSFULLY ===\")\n",
    "print(f\"📁 All files saved to: {export_dir}\")\n",
    "print(f\"📊 Total customers analyzed: {len(enhanced_clv_df):,}\")\n",
    "print(f\"💰 Total portfolio CLV: ${enhanced_clv_df['risk_adjusted_clv'].sum():,.0f}\")\n",
    "print(f\"🎯 High-value customers identified: {len(high_value_customers)}\")\n",
    "print(f\"📈 Model concordance index: {concordance:.3f}\")\n",
    "print(f\"\\n✅ CLV analysis and export complete - ready for optimization and strategic planning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a270c79b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook will implement comprehensive survival analysis and CLV modeling:\n",
    "- Cox proportional hazards model for survival estimation\n",
    "- Expected remaining lifetime calculation for each customer\n",
    "- CLV calculation with proper discounting\n",
    "- Customer segmentation based on CLV\n",
    "- Insights for high-value customer retention strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
